from pathlib import Path

from loguru import logger
from transformers import GPT2TokenizerFast
import typer

PATCH = """


# Automatically generated by vinasmol/training/patch_config.py
# --------------------

vinasmol = dict(
    name="{name}",
    hf_config=dict(org="HuggingFaceTB", name="SmolLM2-360M-Instruct"), # TODO
    block_size=8192,
    vocab_size={vocab_size},
    padded_vocab_size={vocab_size},
    n_layer=32,
    n_head=15,
    n_embd=960,
    n_query_groups=5,
    rotary_percentage=1.0,
    parallel_residual=False,
    bias=False,
    norm_class_name="RMSNorm",
    mlp_class_name="LLaMAMLP",
    intermediate_size=2560,
    rope_base=100000,
    norm_eps=1e-5,
)

configs.append(vinasmol)

name_to_config[vinasmol["name"]] = vinasmol

# --------------------
"""


def main(
        merged_tokenizer_dir: Path = Path("vinasmol/tokenization/data/merged"),
        model_name: str = "VinaSmol-360M",
    ):
    tokenizer: GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(merged_tokenizer_dir)
    new_vocab_size = len(tokenizer) # Includes the added tokens

    logger.info("Adding support for VinaSmol with vocabulary size {}", new_vocab_size)

    import litgpt.config
    file_to_patch = Path(litgpt.config.__file__)

    if model_name not in litgpt.config.name_to_config:
        logger.info("File {} is already patched", file_to_patch)
        return
    
    del litgpt

    python_code = file_to_patch.read_text()
    logger.info("Patching {}", file_to_patch)
    patch = PATCH.format(name=model_name, vocab_size=new_vocab_size)
    file_to_patch.write_text(python_code + patch)



if __name__ == '__main__':
    typer.run(main)