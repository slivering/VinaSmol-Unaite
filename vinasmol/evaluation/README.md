# Evaluation

> [!NOTE]
>  This page is a work in progress.

We evaluate VinaSmol on English and Vietnamese benchmarks. VinaSmol performs much lower than state-of-the-art models but could be competitive with models trained with the same resources.

## English benchmarks

TODO

## SEA benchmarks

We use the individual Vietnamese scores of existing SEA languages benchmarks.

- [SEA-HELM](https://arxiv.org/pdf/2502.14301) ([leaderboard](https://leaderboard.sea-lion.ai/))
- [M3Exam](https://arxiv.org/abs/2306.05179) ([GitHub](https://github.com/DAMO-NLP-SG/M3Exam))

## Vietnamese benchmarks

- [ViLLM-Eval](https://arxiv.org/abs/2404.11086) ([HuggingFace](https://huggingface.co/datasets/vlsp-2023-vllm/ViLLM-Eval), [website](https://ai.stanford.edu/~sttruong/villm/))
- [VMLU](https://vmlu.ai/docs/VMLU_Report_2024.pdf) ([website](https://vmlu.ai/), [leaderboard](https://vmlu.ai/leaderboard))
- [ViGPTQA](https://aclanthology.org/2023.emnlp-industry.70/) ([GitHub](https://github.com/DopikAI-Labs/ViGPT))

## Manual evaluation

If possible, gather feedback from natives, using [OpenWebUI](https://docs.openwebui.com/features/evaluation/)